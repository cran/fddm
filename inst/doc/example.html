<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Kendal Foster and Henrik Singmann" />

<meta name="date" content="2024-06-25" />

<title>Fitting Examples Using fddm</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">
body {
background-color: #ffffff;
max-width: 800px;
margin-left: auto;
margin-right: auto;
margin-top: 0px;
margin-bottom: 0px;
padding-left: 5%;
padding-right: 4%;
padding-top: 10px;
padding-bottom: 30px;
overflow: visible;
font-family: Verdana;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
max-width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: .25em;
margin-bottom: .25em;
}
#TOC ul ul {
margin-left: -2em;
margin-top: 0px;
margin-bottom: 6px;
}
#TOC li {
list-style: disk outside;
line-height: 1.4;
margin-bottom: 5px;
}
#TOC li li {
list-style: circle outside;
line-height: 1.2;
margin-bottom: 0px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
p.method {
background-color: #fcfcfc;
margin-left: 2em;
margin-right: 2em;
border-style: double;
border-width: 4px;
border-color: #b3fffa;
border-radius: 5px;
padding: 0.25em 0.75em 0.25em 1em;
}
span.math {
font-size: 1em;
}
span.eqref{
font-size: 0.75em;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
hr.sec1 {
margin-top: -1.5em;
border-width: 2px;
border-color: #aaaaaa;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: disk outside;
}
ul ul {
margin-bottom: 0;
}
ul ul li {
list-style: circle outside;
}
pre, code {
background-color: #f0f0f0;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 95%;
}
p > code, li > code {
padding: 2px 0px;
}
div.indent2 {
margin-left: 2%;
}
div.indent3 {
margin-left: 3%;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
padding-top: 15px;
font-size: 175%;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #dbdbdb;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #e8e8e8;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5 {
margin-bottom: 0%;
padding-bottom: 0%;
font-size: 110%;
}
h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Fitting Examples Using <code>fddm</code></h1>
<h4 class="author">Kendal Foster and Henrik Singmann</h4>
<h4 class="date">June 25, 2024</h4>



<div id="TOC">
<ul>
<li>
<a href="#intro">Introduction</a>
</li>
<li>
<a href="#ddm">Fitting with <code>ddm()</code></a>
<ul>
<li>
<a href="#ddm-one">Simple Fitting Routine</a>
</li>
</ul>
</li>
<li>
<a href="#dfddm">Fitting Manually with <code>dfddm()</code></a>
<ul>
<li>
<a href="#dfddm-ll-fun">Log-likelihood Function</a>
</li>
<li>
<a href="#dfddm-one">Simple Fitting Routine</a>
</li>
<li>
<a href="#dfddm-all">Fitting the Entire Dataset</a>
</li>
</ul>
</li>
<li>
<a href="#references">References</a>
</li>
</ul>
</div>
<p>Function <code>ddm()</code> fits the 5-parameter Ratcliff diffusion decision model (DDM) via maximum likelihood estimation. The model for each DDM parameter can be specified symbolically using R’s formula interface. With the exception of the drift rate (which is always estimated) all parameters can be either fixed or estimated. As maximum likelihood estimation is based on evaluating the probability density function (PDF), other vignettes in the <code>fddm</code> package address the properties of the function <code>dfddm()</code> that are used to evaluate the PDF. An overview of the mathematical details of the different PDF approximations is provided in the <a href="math.html">Math Vignette</a>. An empirical validation of the implemented PDF methods is provided in the <a href="validity.html">Validity Vignette</a>. Timing benchmarks for the present PDF methods and comparison with existing methods are provided in the <a href="benchmark.html">Benchmark Vignette</a>.</p>
<p>Our implementation of the DDM has the following parameters: <span class="math inline">\(a \in (0, \infty)\)</span> (threshold separation), <span class="math inline">\(v \in (-\infty, \infty)\)</span> (drift rate), <span class="math inline">\(t_0 \in [0, \infty)\)</span> (non-decision time/response time constant), <span class="math inline">\(w \in (0, 1)\)</span> (relative starting point), <span class="math inline">\(sv \in (0, \infty)\)</span> (inter-trial-variability of drift), and <span class="math inline">\(\sigma \in (0, \infty)\)</span> (diffusion coefficient of the underlying Wiener Process). <br><br></p>
<div id="intro" class="section level1">
<h1>Introduction</h1>
<hr class="sec1">
<p>This vignette contains examples of two different ways to use <code>fddm</code> in fitting the DDM to user-supplied real-world data. First, we will demonstrate the use of the <code>ddm()</code> function; we suggest this as the preferred method for most use cases. The <code>ddm()</code> function allows the user to specify which model parameters they want to be estimated and which model parameters they want to remain fixed. Should the user desire more minute control over the fitting procedure, we will show a second method of fitting the DDM that utilizes the exposed likelihood function, <code>dfddm()</code>. This method uses the <code>dfddm()</code> function to construct a log-likelihood function that will be supplied to an optimization routine to estimate each model parameter. The examples that we provide are meant for illustrative purposes, and as such, we will provide a sample analysis for each example.</p>
<p>We will load the <code>fddm::med_dec</code> dataset that is included in the <code>fddm</code> package, and we will use this dataset to fit the Ratcliff DDM in both fitting procedures. This dataset contains the accuracy condition reported in <span class="citation">Trueblood et al. (2018)</span>, which investigates medical decision making among medical professionals (pathologists) and novices (i.e., undergraduate students). The task of participants was to judge whether pictures of blood cells show cancerous cells (i.e., blast cells) or non-cancerous cells (i.e., non-blast cells). The dataset contains 200 decisions per participant, based on pictures of 100 true cancerous cells and pictures of 100 true non-cancerous cells.</p>
<p>Before doing any fitting, we must first load the <code>fddm</code> package, read the data, and clean the data by removing any invalid responses from the data (i.e., have negative or non-numeric response times).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;fddm&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="kw">data</span>(med_dec, <span class="dt">package =</span> <span class="st">&quot;fddm&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>med_dec &lt;-<span class="st"> </span>med_dec[<span class="kw">which</span>(med_dec[[<span class="st">&quot;rt&quot;</span>]] <span class="op">&gt;=</span><span class="st"> </span><span class="dv">0</span>), ]</span></code></pre></div>
<p>As we will be demonstrating simple fitting procedures involving only one participant from the <code>fddm::med_dec</code> dataset, we subset the data to select the individual whose data will be used for these fitting procedures.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>onep &lt;-<span class="st"> </span>med_dec[med_dec[[<span class="st">&quot;id&quot;</span>]] <span class="op">==</span><span class="st"> &quot;2&quot;</span> <span class="op">&amp;</span><span class="st"> </span>med_dec[[<span class="st">&quot;group&quot;</span>]] <span class="op">==</span><span class="st"> &quot;experienced&quot;</span>, ]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="kw">str</span>(onep)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="co">#&gt; &#39;data.frame&#39;:    200 obs. of  9 variables:</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a><span class="co">#&gt;  $ id            : Factor w/ 37 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a><span class="co">#&gt;  $ group         : Factor w/ 3 levels &quot;experienced&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a><span class="co">#&gt;  $ block         : int  3 3 3 3 3 3 3 3 3 3 ...</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a><span class="co">#&gt;  $ trial         : int  1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a><span class="co">#&gt;  $ classification: Factor w/ 2 levels &quot;blast&quot;,&quot;non-blast&quot;: 1 2 2 2 1 1 1 1 2 1 ...</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a><span class="co">#&gt;  $ difficulty    : Factor w/ 2 levels &quot;easy&quot;,&quot;hard&quot;: 1 1 2 2 1 1 2 2 1 2 ...</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a><span class="co">#&gt;  $ response      : Factor w/ 2 levels &quot;blast&quot;,&quot;non-blast&quot;: 1 2 1 2 1 1 1 1 2 1 ...</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a><span class="co">#&gt;  $ rt            : num  0.853 0.575 1.136 0.875 0.748 ...</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a><span class="co">#&gt;  $ stimulus      : Factor w/ 312 levels &quot;blastEasy/AuerRod.jpg&quot;,..: 7 167 246 273 46 31 132 98 217 85 ...</span></span></code></pre></div>
<p>We can see the structure of the data for this participant contains some identifying information (e.g., the ID number and experience level), the enumerated and paired responses and response times, and some information about the stimuli shown to the participant (e.g., the correct classification of the stimulus and the difficulty of correctly identifying the stimulus). We will leverage this extra information about the stimuli in our example fitting procedures.</p>
</div>
<div id="ddm" class="section level1">
<h1>Fitting with <code>ddm()</code></h1>
<hr class="sec1">
<p>The <code>ddm()</code> function fits the 5-parameter DDM to the user-supplied data via maximum likelihood estimation (MLE). Using R’s formula interface, the user can specify which model parameters should be estimated and which should remain fixed; however, the drift rate is always estimated. For the model parameters that the user wishes to be estimated, R’s formula interface allows each parameter to be fit with a single coefficient (i.e., <code>~ 1</code>) or with multiple coefficients (e.g., <code>~ condition1*condition2</code>). Should the user desire a model parameter to be fixed, this can be done by writing the model parameter equal to a scalar (e.g., <code>ndt = 0.39</code>). See the documentation for the <code>ddm()</code> function for more details regarding the formula notation. Since this example is simple, we only use formula notation for the drift rate and leave the other parameters to their default settings.</p>
<div id="ddm-one" class="section level2">
<h2>Simple Fitting Routine</h2>
<p>First we will show a quick fitting using only one participant from the <code>fddm::med_dec</code> dataset. Because we use an ANOVA approach in the analysis of this example, we set orthogonal sum-to-zero contrasts. This means that the “Intercept” coefficient will correspond to the grand mean, and the other coefficients will correspond to the differences from the grand mean.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>opc &lt;-<span class="st"> </span><span class="kw">options</span>(<span class="dt">contrasts =</span> <span class="kw">c</span>(<span class="st">&quot;contr.sum&quot;</span>, <span class="st">&quot;contr.poly&quot;</span>))</span></code></pre></div>
<p>Now we can use the <code>ddm()</code> function to fit the DDM to the data. The first argument of the <code>ddm()</code> function is the formula indicating how the drift rate should be modeled. As the dataset contains both the correct identification of the cell (“classification” column of the dataset, with each row containing the text either “blast” or “non-blast”) and the difficulty of the identification (“difficulty” column of the dataset, with each row containing the text either “easy” or “hard”), we will incorporate this information into how we will model the drift rate. In this case, we will model the drift rate using the grand mean (labeled “Intercept”) with the additional coefficients corresponding to the differences from the grand mean, according to their classification (“blast” or “non-blast”), difficulty (“easy” or “hard”), and the interaction between classification and difficulty. The remaining arguments of the <code>ddm()</code> function contain the formulas (or scalars) for how the other model parameters should be modeled; by default, the boundary separation and non-decision time are estimated by a single coefficient each, and the initial bias and inter-trial variability in the drift rate are held fixed at 0.5 and 0, respectively. Following the model parameters, the remaining arguments are various optimization settings that we will leave as their default values for this example. Note that since we are using the default formulas and scalars for the model paramaters except the drift rate we are not required to explicitly write these formulas and scalars, but we do so in this example for illustrative purposes. We do, however, always need to include the <code>data</code> argument.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>fit0 &lt;-<span class="st"> </span><span class="kw">ddm</span>(<span class="dt">drift =</span> rt <span class="op">+</span><span class="st"> </span>response <span class="op">~</span><span class="st"> </span>classification<span class="op">*</span>difficulty,</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>            <span class="dt">boundary =</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>            <span class="dt">ndt =</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>            <span class="dt">bias =</span> <span class="fl">0.5</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>            <span class="dt">sv =</span> <span class="dv">0</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>            <span class="dt">data =</span> onep)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a><span class="kw">summary</span>(fit0)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a><span class="co">#&gt; Call:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a><span class="co">#&gt; ddm(drift = rt + response ~ classification * difficulty, boundary = ~1, ndt = ~1, bias = 0.5, </span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a><span class="co">#&gt;     sv = 0, data = onep)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a><span class="co">#&gt; DDM fit with 3 estimated and 2 fixed distributional parameters.</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a><span class="co">#&gt; Fixed: bias = 0.5, sv = 0 </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a><span class="co">#&gt; drift coefficients (identity link):</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a><span class="co">#&gt;                             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true"></a><span class="co">#&gt; (Intercept)                   -0.592      0.117   -5.07  3.9e-07 ***</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true"></a><span class="co">#&gt; classification1               -2.645      0.117  -22.65  &lt; 2e-16 ***</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true"></a><span class="co">#&gt; difficulty1                    0.289      0.117    2.47    0.013 *  </span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true"></a><span class="co">#&gt; classification1:difficulty1   -1.499      0.117  -12.83  &lt; 2e-16 ***</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true"></a><span class="co">#&gt; ---</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true"></a><span class="co">#&gt; boundary coefficients (identity link):</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true"></a><span class="co">#&gt;             Estimate Std. Error</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true"></a><span class="co">#&gt; (Intercept)     2.06       0.06</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true"></a><span class="co">#&gt; ndt coefficients (identity link):</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true"></a><span class="co">#&gt;             Estimate Std. Error</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true"></a><span class="co">#&gt; (Intercept)    0.394       0.01</span></span></code></pre></div>
<p>This output first shows the input to the function call and which parameters are held fixed; this information is useful to verify that the formula inputs to the <code>ddm()</code> function call were interpreted as intended by the user. The next line confirms that our model held two parameters fixed (the inital bias and the inter-trial variability in the drift rate) and estimated the other three model parameters. For the model of the drift rate that we input, we can see the estimates and summary statistics for the grand mean (called “Intercept” here) and each difference coefficient (one for the “classification” condition, one for the “difficulty” condition, and one for the interaction between the “classification” and “difficulty” conditions). Below this, we can see the single-coefficient estimates for the boundary separation and non-decision time (default behavior).</p>
<p>For each estimated coefficient, we get the estimate itself and the standard error of the estimate. For the boundary and non-decision time, we can see that the standard errors are pretty small relative to the estimates, so the estimates should be pretty accurate. For the drift rate coefficients, we get additional statistics about the significance of the coefficients- in this case, the grand mean and the differences from the grand mean. From this data, we can see that the effects of “classification”, “difficulty”, and their interaction all meet the common significance requirement P = .05.</p>
<p>We can reset the contrasts after fitting.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="kw">options</span>(opc) <span class="co"># reset contrasts</span></span></code></pre></div>
<div id="ddm-ana" class="section level3">
<h3>Rudimentary Analysis</h3>
<p>For a quick analysis on the drift rate fits, we will do a two-way ANOVA because we are comparing the effect of two categorical variables (classification and density) on a quantitative continuous variable (drift rate). The two-way ANOVA will identify if the drift rate is significantly different among the various classifications and densities; this is a common practice for comparing the effects of categorical variables when two or more groups exist in the data. To be rigorous, we should check that: the data are independent both within groups and across groups; the data are approximately normally distributed, or there are enough observations per group so that we do not need to show normality; the variances across groups are equal; there are no significant outliers; and the data are evenly split among the groups. We will forgo formally verifying these checks for the sake of brevity. Note that the interaction between classification and difficulty is significant, so this term is included in the model.</p>
<p>We will use the <code>emmeans</code> package to do some simple post hoc comparisons across the groups that we modeled; this package has some useful functions to do ANOVA and related stuff. First, we’ll produce an ANOVA-like table that displays the degrees of freedom, F-value, and P-value for each main effect (categorical variable) and their interaction. If we want to show just the conditional main effects, then we can run the second line of code to produce two separate tables with the same summary statistics.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>emmeans<span class="op">::</span><span class="kw">joint_tests</span>(fit0)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="co">#&gt;  model term                df1 df2 F.ratio p.value</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="co">#&gt;  classification              1 194 512.900  &lt;.0001</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a><span class="co">#&gt;  difficulty                  1 194   6.100  0.0142</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a><span class="co">#&gt;  classification:difficulty   1 194 164.700  &lt;.0001</span></span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a>emmeans<span class="op">::</span><span class="kw">joint_tests</span>(fit0, <span class="dt">by =</span> <span class="st">&quot;classification&quot;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="co">#&gt; classification = blast:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a><span class="co">#&gt;  model term df1 df2 F.ratio p.value</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a><span class="co">#&gt;  difficulty   1 194  46.660  &lt;.0001</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a><span class="co">#&gt; classification = non-blast:</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a><span class="co">#&gt;  model term df1 df2 F.ratio p.value</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a><span class="co">#&gt;  difficulty   1 194 137.870  &lt;.0001</span></span></code></pre></div>
<p>From these results, we can see that both of the categorical variables (classification and difficulty) and their interaction term have a significant effect on the drift rate, at the P = 0.05 level.</p>
<p>If we want to see the mean drift rate for each combination of classification and difficulty, the titular <code>emmeans</code> function call will display a table for each classification that contains the standard summary statistics for each difficulty. The following code shows the mean drift rate for each condition in addition to the standard errors, degrees of freedom, and the 95% confidence intervals.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a>em1 &lt;-<span class="st"> </span>emmeans<span class="op">::</span><span class="kw">emmeans</span>(fit0, <span class="st">&quot;difficulty&quot;</span>, <span class="dt">by =</span> <span class="st">&quot;classification&quot;</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a>em1</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a><span class="co">#&gt; classification = blast:</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a><span class="co">#&gt;  difficulty emmean    SE  df lower.CL upper.CL</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true"></a><span class="co">#&gt;  easy       -4.447 0.294 194   -5.026   -3.868</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true"></a><span class="co">#&gt;  hard       -2.027 0.198 194   -2.418   -1.636</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true"></a><span class="co">#&gt; classification = non-blast:</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true"></a><span class="co">#&gt;  difficulty emmean    SE  df lower.CL upper.CL</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true"></a><span class="co">#&gt;  easy        3.840 0.273 194    3.302    4.378</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true"></a><span class="co">#&gt;  hard        0.265 0.135 194   -0.002    0.531</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true"></a><span class="co">#&gt; Confidence level used: 0.95</span></span></code></pre></div>
<p>These means seem to be different; however, we need to formally compare them to be sure. To compare the means across the two difficulty groups (“easy” and “hard”), we can find the pairwise differences between the means (one difference for each classification). The first and second lines of code yield the same information, but the second line condenses the first line from two tables to one table. These tables include the differences between the means as well as the standard summary statistics: standard errors, degrees of freedom, T-statistic, and P-value.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="kw">pairs</span>(em1)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a><span class="co">#&gt; classification = blast:</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a><span class="co">#&gt;  contrast    estimate    SE  df t.ratio p.value</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a><span class="co">#&gt;  easy - hard    -2.42 0.354 194  -6.831  &lt;.0001</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a><span class="co">#&gt; classification = non-blast:</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a><span class="co">#&gt;  contrast    estimate    SE  df t.ratio p.value</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a><span class="co">#&gt;  easy - hard     3.58 0.304 194  11.742  &lt;.0001</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a><span class="kw">update</span>(<span class="kw">pairs</span>(em1), <span class="dt">by =</span> <span class="ot">NULL</span>, <span class="dt">adjust =</span> <span class="st">&quot;holm&quot;</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a><span class="co">#&gt;  contrast    classification estimate    SE  df t.ratio p.value</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true"></a><span class="co">#&gt;  easy - hard blast             -2.42 0.354 194  -6.831  &lt;.0001</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true"></a><span class="co">#&gt;  easy - hard non-blast          3.58 0.304 194  11.742  &lt;.0001</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true"></a><span class="co">#&gt; P value adjustment: holm method for 2 tests</span></span></code></pre></div>
<p>As we suspected, the differences between the means across the two difficulty groups (“easy” and “hard”) are indeed significantly different (at the P = 0.05 level).</p>
</div>
</div>
</div>
<div id="dfddm" class="section level1">
<h1>Fitting Manually with <code>dfddm()</code></h1>
<hr class="sec1">
<p>Although we strongly recommend using the <code>ddm()</code> function for fitting the DDM to data, we will also show an example of how to use the PDF to manually perform the optimization. Note that this method will be slower and less convenient than using the <code>ddm()</code> function, but it is possible if the user wants a particular likelihood function or optimization routine.</p>
<p>Our approach will be a straightforward maximum likelihood estimation (MLE). We are going to be fitting the parameters <span class="math inline">\(v\)</span>, <span class="math inline">\(a\)</span>, <span class="math inline">\(t_0\)</span>, <span class="math inline">\(w\)</span>, and <span class="math inline">\(sv\)</span>; however, we want to fit two distinct drift rates, one for the upper boundary (<span class="math inline">\(v_u\)</span>) and one for the lower boundary (<span class="math inline">\(v_\ell\)</span>). In order to make this distinction, we require the input of the truthful classification of each decision (i.e. what the <em>correct</em> response is for each entry).</p>
<p>Since we will be using the optimization function <code>stats::nlminb()</code>, we must write an objective function for it to optimize; this objective function will be the log-likelihood function that we discuss in the next section.</p>
<div id="dfddm-ll-fun" class="section level2">
<h2>Log-likelihood Function</h2>
<p>By default <code>stats::nlminb()</code> finds the minimum of the objective function instead of the maximum, so we will simply negate our likelihood function. In addition, we will employ the common practice of using the log-likelihood as this tends to be more stable while still maintaining the same minima (negated maxima) as the regular likelihood function. Note that our log-likelihood function depends on the number of response times, the number of responses, and the number of truthful classifications all being equal.</p>
<p>As we are using the optimization function <code>stats::nlminb()</code>, the first argument to our log-likelihood function needs to be a vector of the initial values of the six parameters that are being optimized: <span class="math inline">\(v_u\)</span>, <span class="math inline">\(v_\ell\)</span>, <span class="math inline">\(a\)</span>, <span class="math inline">\(t_0\)</span>, <span class="math inline">\(w\)</span>, and <span class="math inline">\(sv\)</span>. The rest of the arguments will be the other necessary inputs to <code>dfddm()</code> that are not optimized: the vector of response times, the vector of responses, the vector of the truthful classifications, and the allowable error tolerance for the density function (optional). Details on all of these inputs can be found in the <code>dfddm()</code> documentation.</p>
<p>Upon being called, the log-likelihood function first separates the input response times and responses by their truthful classification to yield two new response time vectors and two new response vectors. The response times and responses are then input into separate density functions using a separate <span class="math inline">\(v\)</span> parameter, <span class="math inline">\(v_u\)</span> or <span class="math inline">\(v_\ell\)</span>. These separate densities are then combined, and the log-likelihood function heavily penalizes any combination of parameters that returns a log-density of <span class="math inline">\(-\infty\)</span> (equivalent to a regular density of <span class="math inline">\(0\)</span>). Lastly, the actual log-likelihood is returned as the negative of the sum of all of the log-densities.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a>ll_fun &lt;-<span class="st"> </span><span class="cf">function</span>(pars, rt, resp, truth, err_tol) {</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a>  v &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(rt))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a>  <span class="co"># the truth is &quot;upper&quot; so use vu</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a>  v[truth <span class="op">==</span><span class="st"> &quot;upper&quot;</span>] &lt;-<span class="st"> </span>pars[[<span class="dv">1</span>]]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true"></a>  <span class="co"># the truth is &quot;lower&quot; so use vl</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true"></a>  v[truth <span class="op">==</span><span class="st"> &quot;lower&quot;</span>] &lt;-<span class="st"> </span>pars[[<span class="dv">2</span>]]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true"></a>  dens &lt;-<span class="st"> </span><span class="kw">dfddm</span>(<span class="dt">rt =</span> rt, <span class="dt">response =</span> resp, <span class="dt">a =</span> pars[[<span class="dv">3</span>]], <span class="dt">v =</span> v, <span class="dt">t0 =</span> pars[[<span class="dv">4</span>]],</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true"></a>                <span class="dt">w =</span> pars[[<span class="dv">5</span>]], <span class="dt">sv =</span> pars[[<span class="dv">6</span>]], <span class="dt">err_tol =</span> <span class="fl">1e-6</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true"></a>  <span class="kw">return</span>( <span class="kw">ifelse</span>(<span class="kw">any</span>(<span class="op">!</span><span class="kw">is.finite</span>(dens)), <span class="fl">1e6</span>, <span class="op">-</span><span class="kw">sum</span>(dens)) )</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true"></a>}</span></code></pre></div>
</div>
<div id="dfddm-one" class="section level2">
<h2>Simple Fitting Routine</h2>
<p>First we will fit the DDM to only one participant from the <code>fddm::med_dec</code> dataset. To the single participant’s data we add a column each for converting the responses and their respective classifications to either “upper” or “lower”.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a>onep[[<span class="st">&quot;resp&quot;</span>]] &lt;-<span class="st"> </span><span class="kw">ifelse</span>(onep[[<span class="st">&quot;response&quot;</span>]] <span class="op">==</span><span class="st"> &quot;blast&quot;</span>, <span class="st">&quot;upper&quot;</span>, <span class="st">&quot;lower&quot;</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a>onep[[<span class="st">&quot;truth&quot;</span>]] &lt;-<span class="st"> </span><span class="kw">ifelse</span>(onep[[<span class="st">&quot;classification&quot;</span>]] <span class="op">==</span><span class="st"> &quot;blast&quot;</span>, <span class="st">&quot;upper&quot;</span>, <span class="st">&quot;lower&quot;</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a><span class="kw">str</span>(onep)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true"></a><span class="co">#&gt; &#39;data.frame&#39;:    200 obs. of  11 variables:</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true"></a><span class="co">#&gt;  $ id            : Factor w/ 37 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true"></a><span class="co">#&gt;  $ group         : Factor w/ 3 levels &quot;experienced&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true"></a><span class="co">#&gt;  $ block         : int  3 3 3 3 3 3 3 3 3 3 ...</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true"></a><span class="co">#&gt;  $ trial         : int  1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true"></a><span class="co">#&gt;  $ classification: Factor w/ 2 levels &quot;blast&quot;,&quot;non-blast&quot;: 1 2 2 2 1 1 1 1 2 1 ...</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true"></a><span class="co">#&gt;  $ difficulty    : Factor w/ 2 levels &quot;easy&quot;,&quot;hard&quot;: 1 1 2 2 1 1 2 2 1 2 ...</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true"></a><span class="co">#&gt;  $ response      : Factor w/ 2 levels &quot;blast&quot;,&quot;non-blast&quot;: 1 2 1 2 1 1 1 1 2 1 ...</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true"></a><span class="co">#&gt;  $ rt            : num  0.853 0.575 1.136 0.875 0.748 ...</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true"></a><span class="co">#&gt;  $ stimulus      : Factor w/ 312 levels &quot;blastEasy/AuerRod.jpg&quot;,..: 7 167 246 273 46 31 132 98 217 85 ...</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true"></a><span class="co">#&gt;  $ resp          : chr  &quot;upper&quot; &quot;lower&quot; &quot;upper&quot; &quot;lower&quot; ...</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true"></a><span class="co">#&gt;  $ truth         : chr  &quot;upper&quot; &quot;lower&quot; &quot;lower&quot; &quot;lower&quot; ...</span></span></code></pre></div>
<p>We pass the data and log-likelihood function with the necessary additional arguments to an optimization function. As we are using the optimization function <code>stats::nlminb()</code> for this example, we must input as the first argument the initial values of our DDM parameters that we want optimized. These are input in the order: <span class="math inline">\(v_u\)</span>, <span class="math inline">\(v_\ell\)</span>, <span class="math inline">\(a\)</span>, <span class="math inline">\(t_0\)</span>, <span class="math inline">\(w\)</span>, and <span class="math inline">\(sv\)</span>; we also need to define upper and lower bounds for each parameters.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">nlminb</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">0</span>), <span class="dt">objective =</span> ll_fun,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a>              <span class="dt">rt =</span> onep[[<span class="st">&quot;rt&quot;</span>]], <span class="dt">resp =</span> onep[[<span class="st">&quot;resp&quot;</span>]], <span class="dt">truth =</span> onep[[<span class="st">&quot;truth&quot;</span>]],</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a>              <span class="co"># limits:   vu,   vl,   a,  t0, w,  sv</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true"></a>              <span class="dt">lower =</span> <span class="kw">c</span>(<span class="op">-</span><span class="ot">Inf</span>, <span class="op">-</span><span class="ot">Inf</span>, <span class="fl">.01</span>,   <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span>),</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true"></a>              <span class="dt">upper =</span> <span class="kw">c</span>( <span class="ot">Inf</span>,  <span class="ot">Inf</span>, <span class="ot">Inf</span>, <span class="ot">Inf</span>, <span class="dv">1</span>, <span class="ot">Inf</span>))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true"></a>fit</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true"></a><span class="co">#&gt; $par</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true"></a><span class="co">#&gt; [1]  5.6813 -2.1887  2.7909  0.3764  0.4010  2.2813</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true"></a><span class="co">#&gt; $objective</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true"></a><span class="co">#&gt; [1] 42.47</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true"></a><span class="co">#&gt; $convergence</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true"></a><span class="co">#&gt; [1] 0</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true"></a><span class="co">#&gt; $iterations</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true"></a><span class="co">#&gt; [1] 131</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true"></a><span class="co">#&gt; $evaluations</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true"></a><span class="co">#&gt; function gradient </span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true"></a><span class="co">#&gt;      157      932 </span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true"></a><span class="co">#&gt; $message</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true"></a><span class="co">#&gt; [1] &quot;relative convergence (4)&quot;</span></span></code></pre></div>
</div>
<div id="dfddm-all" class="section level2">
<h2>Fitting the Entire Dataset</h2>
<p>Here we will run a more rigorous fitting on the entire <code>fddm::med_dec</code> dataset to obtain parameter estimates for each participant in the study. To do this, we define a function to run the data fitting for us; we want it to output a dataframe containing the parameter estimates for each individual in the data. The inputs will be the dataset, the allowable error tolerance for the density function, how the “upper” response is presented in the dataset, and indices of the columns in the dataset containing: identification of the individuals in the dataset, the response times, the responses, and the truthful classifications.</p>
<p>After some data checking, the fitting function will extract the unique individuals from the dataset and run the parameter optimization for the responses and response times for each individual. The optimizations themselves are initialized with random initial parameter values to aid in the avoidance of local minima in favor of global minima. Moreover, the optimization will run 5 times for each individual, with 5 different sets of random initial parameter values. The value of the minimized log-likelihood function will be compared across all 5 runs, and the smallest such value will indicate the best fit. The parameter estimates, convergence code, and minimized value of the log-likelihood function produced by this best fit will be saved for that individual.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a>rt_fit &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">id_idx =</span> <span class="ot">NULL</span>, <span class="dt">rt_idx =</span> <span class="ot">NULL</span>, <span class="dt">response_idx =</span> <span class="ot">NULL</span>,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true"></a>                   <span class="dt">truth_idx =</span> <span class="ot">NULL</span>, <span class="dt">response_upper =</span> <span class="ot">NULL</span>) {</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true"></a>  <span class="co"># Format data for fitting</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true"></a>  <span class="cf">if</span> (<span class="kw">all</span>(<span class="kw">is.null</span>(id_idx), <span class="kw">is.null</span>(rt_idx), <span class="kw">is.null</span>(response_idx),</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true"></a>      <span class="kw">is.null</span>(truth_idx), <span class="kw">is.null</span>(response_upper))) {</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true"></a>    df &lt;-<span class="st"> </span>data <span class="co"># assume input data is already formatted</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true"></a>  } <span class="cf">else</span> {</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true"></a>    <span class="cf">if</span>(<span class="kw">any</span>(data[,rt_idx] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)) {</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true"></a>      <span class="kw">stop</span>(<span class="st">&quot;Input data contains negative response times; fit will not be run.&quot;</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true"></a>    }</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true"></a>    <span class="cf">if</span>(<span class="kw">any</span>(<span class="kw">is.na</span>(data[,response_idx]))) {</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true"></a>      <span class="kw">stop</span>(<span class="st">&quot;Input data contains invalid responses (NA); fit will not be run.&quot;</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true"></a>    }</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true"></a>    nr &lt;-<span class="st"> </span><span class="kw">nrow</span>(data)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true"></a>    df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> <span class="kw">character</span>(nr),</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true"></a>                     <span class="dt">rt =</span> <span class="kw">double</span>(nr),</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true"></a>                     <span class="dt">response =</span> <span class="kw">character</span>(nr),</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true"></a>                     <span class="dt">truth =</span> <span class="kw">character</span>(nr),</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true"></a>                     <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true"></a>    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(id_idx)) { <span class="co"># relabel identification tags</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true"></a>      <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(id_idx)) {</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true"></a>        idi &lt;-<span class="st"> </span><span class="kw">unique</span>(data[,id_idx[i]])</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true"></a>        <span class="cf">for</span> (j <span class="cf">in</span> <span class="kw">seq_along</span>(idi)) {</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true"></a>          df[[<span class="st">&quot;id&quot;</span>]][data[,id_idx[i]] <span class="op">==</span><span class="st"> </span>idi[j]] &lt;-<span class="st"> </span><span class="kw">paste</span>(</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true"></a>            df[[<span class="st">&quot;id&quot;</span>]][data[,id_idx[i]] <span class="op">==</span><span class="st"> </span>idi[j]], idi[j], <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true"></a>        }</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true"></a>      }</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true"></a>      df[[<span class="st">&quot;id&quot;</span>]] &lt;-<span class="st"> </span><span class="kw">trimws</span>(df[[<span class="st">&quot;id&quot;</span>]], <span class="dt">which =</span> <span class="st">&quot;left&quot;</span>)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true"></a>    }</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true"></a>    df[[<span class="st">&quot;rt&quot;</span>]] &lt;-<span class="st"> </span><span class="kw">as.double</span>(data[,rt_idx])</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true"></a>    df[[<span class="st">&quot;response&quot;</span>]] &lt;-<span class="st"> &quot;lower&quot;</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true"></a>    df[[<span class="st">&quot;response&quot;</span>]][data[,response_idx] <span class="op">==</span><span class="st"> </span>response_upper] &lt;-<span class="st"> &quot;upper&quot;</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true"></a>    df[[<span class="st">&quot;truth&quot;</span>]] &lt;-<span class="st"> &quot;lower&quot;</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true"></a>    df[[<span class="st">&quot;truth&quot;</span>]][data[,truth_idx] <span class="op">==</span><span class="st"> </span>response_upper] &lt;-<span class="st"> &quot;upper&quot;</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true"></a>  }</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true"></a></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true"></a>  <span class="co"># Preliminaries</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true"></a>  ids &lt;-<span class="st"> </span><span class="kw">unique</span>(df[[<span class="st">&quot;id&quot;</span>]])</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true"></a>  nids &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">length</span>(ids), <span class="dv">1</span>) <span class="co"># if inds is null, there is only one individual</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true"></a>  ninit_vals &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true"></a>  <span class="co"># Initilize the output dataframe</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true"></a>  cnames &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;Convergence&quot;</span>, <span class="st">&quot;Objective&quot;</span>,</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true"></a>              <span class="st">&quot;vu_fit&quot;</span>, <span class="st">&quot;vl_fit&quot;</span>, <span class="st">&quot;a_fit&quot;</span>, <span class="st">&quot;t0_fit&quot;</span>, <span class="st">&quot;w_fit&quot;</span>, <span class="st">&quot;sv_fit&quot;</span>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true"></a>  out &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="kw">length</span>(cnames), <span class="dt">nrow =</span> nids))</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true"></a>  <span class="kw">colnames</span>(out) &lt;-<span class="st"> </span>cnames</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true"></a>  temp &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="kw">length</span>(cnames)<span class="op">-</span><span class="dv">1</span>, <span class="dt">nrow =</span> ninit_vals))</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true"></a>  <span class="kw">colnames</span>(temp) &lt;-<span class="st"> </span>cnames[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true"></a>  <span class="co"># Loop through each individual and starting values</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nids) {</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true"></a>    out[[<span class="st">&quot;ID&quot;</span>]][i] &lt;-<span class="st"> </span>ids[i]</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true"></a>    <span class="co"># extract data for id i</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true"></a>    dfi &lt;-<span class="st"> </span>df[df[[<span class="st">&quot;id&quot;</span>]] <span class="op">==</span><span class="st"> </span>ids[i],]</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true"></a>    rti &lt;-<span class="st"> </span>dfi[[<span class="st">&quot;rt&quot;</span>]]</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true"></a>    respi &lt;-<span class="st"> </span>dfi[[<span class="st">&quot;response&quot;</span>]]</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true"></a>    truthi &lt;-<span class="st"> </span>dfi[[<span class="st">&quot;truh&quot;</span>]]</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true"></a></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true"></a>    <span class="co"># starting value for t0 must be smaller than the smallest rt</span></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true"></a>    min_rti &lt;-<span class="st"> </span><span class="kw">min</span>(rti)</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true"></a>    <span class="co"># create initial values for this individual</span></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true"></a>    init_vals &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">vu =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> ninit_vals, <span class="dt">mean =</span> <span class="dv">4</span>, <span class="dt">sd =</span> <span class="dv">2</span>),</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true"></a>                            <span class="dt">vl =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> ninit_vals, <span class="dt">mean =</span> <span class="dv">-4</span>, <span class="dt">sd =</span> <span class="dv">2</span>),</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true"></a>                            <span class="dt">a  =</span> <span class="kw">runif</span>(<span class="dt">n =</span> ninit_vals, <span class="dt">min =</span> <span class="fl">0.5</span>, <span class="dt">max =</span> <span class="dv">5</span>),</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true"></a>                            <span class="dt">t0 =</span> <span class="kw">runif</span>(<span class="dt">n =</span> ninit_vals, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> min_rti),</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true"></a>                            <span class="dt">w  =</span> <span class="kw">runif</span>(<span class="dt">n =</span> ninit_vals, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>),</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true"></a>                            <span class="dt">sv =</span> <span class="kw">runif</span>(<span class="dt">n =</span> ninit_vals, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">5</span>))</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true"></a></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true"></a>    <span class="co"># loop through all of the starting values</span></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>ninit_vals) {</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true"></a>      mres &lt;-<span class="st"> </span><span class="kw">nlminb</span>(init_vals[j,], ll_fun,</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true"></a>                     <span class="dt">rt =</span> rti, <span class="dt">resp =</span> respi, <span class="dt">truth =</span> truthi,</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true"></a>                     <span class="co"># limits:   vu,   vl,   a,  t0, w,  sv</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true"></a>                     <span class="dt">lower =</span> <span class="kw">c</span>(<span class="op">-</span><span class="ot">Inf</span>, <span class="op">-</span><span class="ot">Inf</span>, <span class="fl">.01</span>,   <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span>),</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true"></a>                     <span class="dt">upper =</span> <span class="kw">c</span>( <span class="ot">Inf</span>,  <span class="ot">Inf</span>, <span class="ot">Inf</span>, <span class="ot">Inf</span>, <span class="dv">1</span>, <span class="ot">Inf</span>))</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true"></a>      temp[[<span class="st">&quot;Convergence&quot;</span>]][j] &lt;-<span class="st"> </span>mres[[<span class="st">&quot;convergence&quot;</span>]]</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true"></a>      temp[[<span class="st">&quot;Objective&quot;</span>]][j] &lt;-<span class="st"> </span>mres[[<span class="st">&quot;objective&quot;</span>]]</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true"></a>      temp[j, <span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)] &lt;-<span class="st"> </span>mres[[<span class="st">&quot;par&quot;</span>]]</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true"></a>    }</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true"></a></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true"></a>    <span class="co"># determine best fit for the individual</span></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true"></a>    min_idx &lt;-<span class="st"> </span><span class="kw">which.min</span>(temp[[<span class="st">&quot;Objective&quot;</span>]])</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true"></a>    out[i, <span class="dv">-1</span>] &lt;-<span class="st"> </span>temp[min_idx,]</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true"></a>  }</span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true"></a>  <span class="kw">return</span>(out)</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true"></a>}</span></code></pre></div>
<p>We run the fitting, and the dataframe of the fitting results is output below.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">rt_fit</span>(med_dec, <span class="dt">id_idx =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="dt">rt_idx =</span> <span class="dv">8</span>, <span class="dt">response_idx =</span> <span class="dv">7</span>,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true"></a>              <span class="dt">truth_idx =</span> <span class="dv">5</span>, <span class="dt">response_upper =</span> <span class="st">&quot;blast&quot;</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true"></a>fit</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true"></a><span class="co">#&gt;                  ID Convergence Objective  vu_fit   vl_fit  a_fit t0_fit  w_fit    sv_fit</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true"></a><span class="co">#&gt; 1     experienced 2           0   154.157  2.7128 -3.78438 2.6092 0.4151 0.4993 5.805e+00</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true"></a><span class="co">#&gt; 2     experienced 6           0   119.675  4.8240 -7.28773 2.3638 0.3739 0.5930 5.380e+00</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true"></a><span class="co">#&gt; 3     experienced 7           0    74.397  6.3357 -2.48671 1.3876 0.4535 0.4499 1.945e+00</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true"></a><span class="co">#&gt; 4     experienced 9           0   162.764  3.3282 -1.14510 2.5723 0.4724 0.4730 5.316e+00</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true"></a><span class="co">#&gt; 5    experienced 12           0   122.762  5.9373 -4.61748 2.1233 0.3871 0.5431 4.559e+00</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true"></a><span class="co">#&gt; 6    experienced 14           0   107.097  7.4802 -2.86249 1.5003 0.4060 0.5481 1.785e+00</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true"></a><span class="co">#&gt; 7    experienced 16           0   316.742  5.1376 -3.85954 2.0504 0.5071 0.4397 2.766e-01</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true"></a><span class="co">#&gt; 8    experienced 17           0   238.977  4.8629 -6.32497 2.2852 0.4859 0.4117 2.462e+00</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true"></a><span class="co">#&gt; 9   inexperienced 3           0    90.848  2.6116 -3.41934 1.2960 0.4461 0.6715 1.593e-09</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true"></a><span class="co">#&gt; 10  inexperienced 4           0   186.478  2.7666 -0.01614 1.5964 0.4145 0.4630 7.141e-01</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true"></a><span class="co">#&gt; 11  inexperienced 5           0   182.976  4.2869 -6.15060 2.8352 0.4174 0.3807 5.718e+00</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true"></a><span class="co">#&gt; 12  inexperienced 8           0   219.081  6.8420 -7.83934 2.0509 0.1464 0.6034 7.728e-01</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true"></a><span class="co">#&gt; 13 inexperienced 10           0    70.452  4.5687 -6.64514 1.4830 0.4132 0.4507 3.113e+00</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true"></a><span class="co">#&gt; 14 inexperienced 11           0   268.962  6.6532 -2.88401 2.4425 0.1317 0.5135 1.300e+00</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true"></a><span class="co">#&gt; 15 inexperienced 13           0   129.581  3.6832 -5.28313 2.0976 0.3946 0.6284 4.207e+00</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true"></a><span class="co">#&gt; 16 inexperienced 15           0    83.415  4.7988 -4.28267 1.2326 0.3726 0.5849 2.245e-06</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true"></a><span class="co">#&gt; 17 inexperienced 18           0   119.633  2.1845 -1.88091 1.4366 0.5222 0.4980 1.559e+00</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true"></a><span class="co">#&gt; 18 inexperienced 19           0   233.401  3.9174 -5.18082 2.6755 0.3876 0.4712 3.926e+00</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true"></a><span class="co">#&gt; 19         novice 1           0   169.743  3.8978 -7.42682 1.6938 0.3987 0.4472 1.132e+00</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true"></a><span class="co">#&gt; 20         novice 2           0   234.286  4.0355 -5.80434 1.7176 0.5541 0.3470 6.200e-08</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true"></a><span class="co">#&gt; 21         novice 3           0   157.031  1.9510 -2.95794 1.3439 0.4973 0.5263 2.450e-08</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true"></a><span class="co">#&gt; 22         novice 4           0    89.457  3.2892 -1.99313 1.3713 0.3775 0.4943 1.274e+00</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true"></a><span class="co">#&gt; 23         novice 5           0   173.613  3.6232 -2.38817 1.4204 0.5130 0.4725 5.211e-08</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true"></a><span class="co">#&gt; 24         novice 6           0   142.353  3.7401 -2.07636 1.5024 0.4409 0.5888 8.112e-01</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true"></a><span class="co">#&gt; 25         novice 7           0   372.724  5.5536 -3.77155 2.4687 0.4413 0.3790 2.596e-08</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true"></a><span class="co">#&gt; 26         novice 8           0    77.527  3.3908 -2.25729 1.4522 0.1090 0.6044 8.038e-01</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true"></a><span class="co">#&gt; 27         novice 9           0    62.579  7.3772 -2.59243 1.2418 0.3777 0.4889 1.193e+00</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true"></a><span class="co">#&gt; 28        novice 10           0   100.287  1.4143 -4.54823 1.2963 0.4663 0.4521 1.270e+00</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true"></a><span class="co">#&gt; 29        novice 11           0    77.262  6.3395 -5.15154 1.4512 0.3841 0.6106 2.072e+00</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true"></a><span class="co">#&gt; 30        novice 12           0    85.624  4.9233 -5.93745 1.4676 0.3523 0.3428 5.193e-09</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true"></a><span class="co">#&gt; 31        novice 13           0    84.946  5.1602 -3.40858 1.4569 0.3979 0.6826 1.265e+00</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true"></a><span class="co">#&gt; 32        novice 14           0    72.945  5.2018 -3.23745 1.6538 0.3755 0.4775 4.044e+00</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true"></a><span class="co">#&gt; 33        novice 15           0   149.422  2.1252 -3.40530 1.9703 0.4065 0.4891 3.531e+00</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true"></a><span class="co">#&gt; 34        novice 16           0   150.412  2.0545 -4.85879 1.7086 0.3357 0.6148 1.435e+00</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true"></a><span class="co">#&gt; 35        novice 17           0   264.390  4.1305 -6.85353 2.3208 0.1914 0.4617 1.132e+00</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true"></a><span class="co">#&gt; 36        novice 18           0   104.866  6.0616 -3.53141 1.3891 0.4650 0.4954 1.399e+00</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true"></a><span class="co">#&gt; 37        novice 19           0   209.592  3.1368 -3.16940 2.1568 0.4349 0.5002 2.472e+00</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true"></a><span class="co">#&gt; 38        novice 20           0   234.731  4.7427 -5.31081 2.0437 0.0000 0.5734 6.434e-01</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true"></a><span class="co">#&gt; 39        novice 21           0     8.786  6.2172 -4.16456 1.0759 0.4179 0.5128 1.384e+00</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true"></a><span class="co">#&gt; 40        novice 22           0    57.884  0.3521 -2.40516 1.3349 0.3313 0.5455 2.219e+00</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true"></a><span class="co">#&gt; 41        novice 23           0   112.185 -1.9239 -5.45614 1.3942 0.4165 0.5527 1.358e+00</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true"></a><span class="co">#&gt; 42        novice 24           0    98.604  5.7574 -2.29942 1.6113 0.3361 0.6214 1.385e+00</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true"></a><span class="co">#&gt; 43        novice 25           0   101.399  4.6463 -3.98581 1.5317 0.4189 0.4984 2.184e+00</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true"></a><span class="co">#&gt; 44        novice 26           0   187.189  6.2760 -2.56965 1.6987 0.4637 0.3486 6.712e-01</span></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true"></a><span class="co">#&gt; 45        novice 27           0    64.849  2.4300 -4.20083 1.2238 0.5150 0.4185 1.101e+00</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true"></a><span class="co">#&gt; 46        novice 28           0   143.266  0.7633 -4.21327 1.6261 0.1316 0.4763 9.851e-01</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true"></a><span class="co">#&gt; 47        novice 29           0    -8.025  1.3266 -2.66957 0.9979 0.3695 0.5096 1.222e+00</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true"></a><span class="co">#&gt; 48        novice 30           0   131.602  0.9789 -2.81143 1.5847 0.3331 0.5513 1.135e+00</span></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true"></a><span class="co">#&gt; 49        novice 31           0   232.037  3.8640 -3.33077 1.8749 0.4700 0.4490 1.079e+00</span></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true"></a><span class="co">#&gt; 50        novice 32           0   101.678  4.7039 -3.48549 1.3497 0.5102 0.4355 1.523e+00</span></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true"></a><span class="co">#&gt; 51        novice 33           0   111.875  6.0257 -4.50788 1.2291 0.3395 0.5337 2.722e-06</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true"></a><span class="co">#&gt; 52        novice 34           0    99.261  3.2596 -2.82126 1.2717 0.4964 0.5752 5.818e-01</span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true"></a><span class="co">#&gt; 53        novice 35           0   176.715  5.4185 -5.34192 2.1075 0.5099 0.4705 3.545e+00</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true"></a><span class="co">#&gt; 54        novice 36           0    92.863  3.3667 -3.90339 1.3239 0.5260 0.4143 1.049e+00</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true"></a><span class="co">#&gt; 55        novice 37           0   101.640  2.5829 -4.45512 1.4562 0.3345 0.6031 9.149e-01</span></span></code></pre></div>
<div id="dfddm-all-ana" class="section level3">
<h3>Rudimentary Analysis</h3>
<p>To show some basic results of our fitting, we will plot the fitted values of <span class="math inline">\(v_u\)</span> and <span class="math inline">\(v_\ell\)</span> grouped by the experience level of the participant to demonstrate how these parameters differ among novices, inexperienced professionals, and experienced professionals.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;reshape2&quot;</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true"></a>fitp &lt;-<span class="st"> </span><span class="kw">data.frame</span>(fit[, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">5</span>)]) <span class="co"># make a copy to manipulate for plotting</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true"></a><span class="kw">colnames</span>(fitp)[<span class="op">-</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;vu&quot;</span>, <span class="st">&quot;vl&quot;</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(<span class="kw">unique</span>(fitp[[<span class="st">&quot;ID&quot;</span>]]))) {</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true"></a>  first &lt;-<span class="st"> </span><span class="kw">substr</span>(fitp[[<span class="st">&quot;ID&quot;</span>]][i], <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true"></a>  <span class="cf">if</span> (first <span class="op">==</span><span class="st"> &quot;n&quot;</span>) {</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true"></a>    fitp[[<span class="st">&quot;ID&quot;</span>]][i] &lt;-<span class="st"> &quot;novice&quot;</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true"></a>  } <span class="cf">else</span> <span class="cf">if</span> (first <span class="op">==</span><span class="st"> &quot;i&quot;</span>) {</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true"></a>    fitp[[<span class="st">&quot;ID&quot;</span>]][i] &lt;-<span class="st"> &quot;inexperienced&quot;</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true"></a>  } <span class="cf">else</span> {</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true"></a>    fitp[[<span class="st">&quot;ID&quot;</span>]][i] &lt;-<span class="st"> &quot;experienced&quot;</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true"></a>  }</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true"></a>}</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true"></a>fitp &lt;-<span class="st"> </span><span class="kw">melt</span>(fitp, <span class="dt">id.vars =</span> <span class="st">&quot;ID&quot;</span>, <span class="dt">measure.vars =</span> <span class="kw">c</span>(<span class="st">&quot;vu&quot;</span>, <span class="st">&quot;vl&quot;</span>),</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true"></a>             <span class="dt">variable.name =</span> <span class="st">&quot;vuvl&quot;</span>, <span class="dt">value.name =</span> <span class="st">&quot;estimate&quot;</span>)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true"></a><span class="kw">ggplot</span>(fitp, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(ID, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;novice&quot;</span>, <span class="st">&quot;inexperienced&quot;</span>, <span class="st">&quot;experienced&quot;</span>)),</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true"></a>                 <span class="dt">y =</span> estimate,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true"></a>                 <span class="dt">color =</span> <span class="kw">factor</span>(vuvl, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;vu&quot;</span>, <span class="st">&quot;vl&quot;</span>)))) <span class="op">+</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.4</span>, <span class="dt">size =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Parameter Estimates for vu and vl&quot;</span>,</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true"></a>       <span class="dt">x =</span> <span class="st">&quot;Experience Level&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Parameter Estimate&quot;</span>,</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true"></a>       <span class="dt">color =</span> <span class="st">&quot;Drift Rate&quot;</span>) <span class="op">+</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.border =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true"></a>        <span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">23</span>),</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true"></a>        <span class="dt">plot.subtitle =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>),</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true"></a>        <span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>),</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true"></a>        <span class="dt">axis.text.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>),</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true"></a>        <span class="dt">axis.title.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">20</span>,</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true"></a>                                    <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true"></a>        <span class="dt">axis.title.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">20</span>),</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true"></a>        <span class="dt">legend.title =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">20</span>),</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true"></a>        <span class="dt">legend.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>))</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Before we begin analysis of this plot, note that the drift rate corresponding to the upper threshold should always be positive, and the drift rate corresponding to the lower threshold should always be negative. Since there are a few fitted values that switch this convention, the novice participants show evidence of consistently responding incorrectly to the stimulus. In contrast, both the inexperienced and experienced participants show a clean division of drift rates around zero.</p>
<p>In addition, we notice that the more experienced participants tend to have higher fitted drift rates in absolute value. A more extreme drift rate means that the participant receives and processes information more efficiently than a more mild drift rate. The overall pattern is that the novices are on average the worst at receiving information, the experienced professionals are the best, and the inexperienced professionals are somewhere in the middle. This pattern indicates that experienced professionals are indeed better at their job than untrained undergraduate students!</p>
</div>
</div>
</div>
</div>
<div id="section" class="section level1 unlisted unnumbered">
<h1 class="unlisted unnumbered"></h1>
<div id="r-session-info" class="section level4 unlisted unnumbered">
<h4 class="unlisted unnumbered">R Session Info</h4>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a><span class="kw">sessionInfo</span>()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true"></a><span class="co">#&gt; R version 4.4.1 (2024-06-14)</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true"></a><span class="co">#&gt; Platform: x86_64-pc-linux-gnu</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true"></a><span class="co">#&gt; Running under: Linux Mint 21.3</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true"></a><span class="co">#&gt; Matrix products: default</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true"></a><span class="co">#&gt; BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 </span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true"></a><span class="co">#&gt; LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true"></a><span class="co">#&gt; locale:</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true"></a><span class="co">#&gt;  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8       </span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true"></a><span class="co">#&gt;  [4] LC_COLLATE=C               LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   </span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true"></a><span class="co">#&gt;  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C              </span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true"></a><span class="co">#&gt; [10] LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       </span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true"></a><span class="co">#&gt; time zone: Europe/London</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true"></a><span class="co">#&gt; tzcode source: system (glibc)</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true"></a><span class="co">#&gt; attached base packages:</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true"></a><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true"></a><span class="co">#&gt; other attached packages:</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true"></a><span class="co">#&gt; [1] ggforce_0.4.2         ggplot2_3.5.1         reshape2_1.4.4        microbenchmark_1.4.10</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true"></a><span class="co">#&gt; [5] RWiener_1.3-3         rtdists_0.11-5        fddm_1.0-1           </span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true"></a><span class="co">#&gt; </span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true"></a><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true"></a><span class="co">#&gt;  [1] sass_0.4.9         utf8_1.2.4         generics_0.1.3     stringi_1.8.4      lattice_0.22-6    </span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true"></a><span class="co">#&gt;  [6] digest_0.6.35      magrittr_2.0.3     estimability_1.5.1 evaluate_0.24.0    grid_4.4.1        </span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true"></a><span class="co">#&gt; [11] mvtnorm_1.2-5      fastmap_1.2.0      plyr_1.8.9         jsonlite_1.8.8     Matrix_1.7-0      </span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true"></a><span class="co">#&gt; [16] ggnewscale_0.4.10  Formula_1.2-5      survival_3.7-0     fansi_1.0.6        scales_1.3.0      </span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true"></a><span class="co">#&gt; [21] tweenr_2.0.3       codetools_0.2-20   jquerylib_0.1.4    cli_3.6.2          rlang_1.1.4       </span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true"></a><span class="co">#&gt; [26] expm_0.999-9       polyclip_1.10-6    gsl_2.1-8          munsell_0.5.1      splines_4.4.1     </span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true"></a><span class="co">#&gt; [31] withr_3.0.0        cachem_1.1.0       yaml_2.3.8         tools_4.4.1        dplyr_1.1.4       </span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true"></a><span class="co">#&gt; [36] colorspace_2.1-0   vctrs_0.6.5        R6_2.5.1           emmeans_1.10.2     lifecycle_1.0.4   </span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true"></a><span class="co">#&gt; [41] stringr_1.5.1      MASS_7.3-61        pkgconfig_2.0.3    pillar_1.9.0       bslib_0.7.0       </span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true"></a><span class="co">#&gt; [46] gtable_0.3.5       glue_1.7.0         Rcpp_1.0.12        highr_0.11         xfun_0.45         </span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true"></a><span class="co">#&gt; [51] tibble_3.2.1       tidyselect_1.2.1   knitr_1.47         msm_1.7.1          xtable_1.8-4      </span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true"></a><span class="co">#&gt; [56] farver_2.1.2       htmltools_0.5.8.1  labeling_0.4.3     rmarkdown_2.27     compiler_4.4.1    </span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true"></a><span class="co">#&gt; [61] evd_2.3-7</span></span></code></pre></div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-trueblood_impact_2018">
<p>Trueblood, Jennifer S., William R. Holmes, Adam C. Seegmiller, Jonathan Douds, Margaret Compton, Eszter Szentirmai, Megan Woodruff, Wenrui Huang, Charles Stratton, and Quentin Eichbaum. 2018. “The Impact of Speed and Bias on the Cognitive Processes of Experts and Novices in Medical Image Decision-Making.” <em>Cognitive Research: Principles and Implications</em> 3 (1): 28. <a href="https://doi.org/10.1186/s41235-018-0119-2">https://doi.org/10.1186/s41235-018-0119-2</a>.</p>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
